 
 # P vs NP: Do better machines exist?


 Pondering on the limits of computing, there are several hard problems which can only be thought of as fascinating; some of them cannot be solved and we know the negative answer for sure (the halting problem), and some of them are so alien and exotic that can't even be framed with the math we currently possess. Which is the case for assessing wether very hard problems (non polynomial: NP) can be solved with a meta technique to generate all posible clever cheap tricks (polynomial: P) to solve the problem. The general answer so far is neither positive nor negative, because proof for either case doesent exist. P VS NP is one of the most profound and fundamental math and computer problems, and while the general sentiment is that naturally the answer can only be negative; a positive answer would completely reshape the world - not just computers, but the landscape of reality as we know it. For this reason, is one of the seven millenium problems, and IMO it might be the last problem to be solved if we ever do so.

 While the most prominent complexity techniques like diagonlization cannot posit a good frame for the problem and help build a proof, theres certainly different ways in which P vs NP can be casted down and be manipulated from a different perspective. Down to the problems essence you get the Universal Turing Machine with its tape, execution head, transition function and a control unit to act as working memory; this elements give the machine the faculty to tackle in a definite ammount of time a specification provided an input. Furhermore these four elements come together and propose a mathematical artifact that is capable of computing any sequence of symbols provided an algorithmic specification for parsing. The Turing Machine, a more specific abd watered down version of the UTM, is quite possibly the apex of human ingenuity, and the foundation upon which western civilization rests since the XX century digital and industrial revolution.

 Now after a decade ruminiating the problem (was 19 when i was first intrduced to it), i came to understand that the starting point for the new math that may posibly make P vs NP approachable starts from looking into three fields for new "tools":

 1) Cybernetics
 2) Systems philosophy
 3) Spiking neural networks

Back when he was alive i used to be a follower of Terry A Davis - besides his particularities which made him difficult to work with (no qualms in randomly using the N word - he was mentally unwell) he had a fixation on random numbers which i found interesting. He would often prompt quantum random number generators expecting some emergence of order to come out of them, and hand like an Oracle"an interesting message from God". Sometimes the prompts where very interesting and actually predicted stuff (kinda); but most of the times the prompts where all duds - much to his own sadness, as he admited that God wasnt feeling like talking that day.

https://www.youtube.com/watch?v=_QccgKS4b58

Anyway. Terry esentially had an obssesive fixation on retrocausality, and retrocausality happens to be the foundation upon which most supernatural phenomena rests upon. Esentially retrocausality can be framed as a way of looking ahead in a queue to see whats coming next before it happens. People with intuitive dispositions, can get a sense that somethings wrong by the measure of how their unconscious starts to get signals ahead of time that something bad is about to happen. You just get a bad feeling about something, basically. 

So how does this relate to P vs NP at all?

Well i think that a new framework can be dissected from some points i gathered over the years.

1) The hardness of a problem is subject to the maximal computational capacity of the subject that is actively framing the problem. The problem inversely mirrors the solver.

2) Compounded biological neural networks are resource efficient when compared to the expensive 8 gb VRAM trash NVIDIA produces; and in some cases capable of manipulating data across dimensions of complexity by "switching" the framing of the problem. Biological neural nets can forget and self destroy their initial structure in favour of a more efficient one.

3) The first approximation and computation attempted of the problem is critical (like a bias in a neural net, setting itself ahead for a plausible intial specialization), so an unfit unknown algorithm will exhaust its fitness just by even _looking_ at the problem. Corruption ensues. All machines loop and bounce back into themselves when tasked with an application that doesent match perfectly their specification.


So we cannot do anything about 1) but points two and three are esentially pinpointing an absence in the current UTM model. First a sleeping UTM as i call them has to be relatively "blind" to previous specifications so it doesent corrupt the frame of the problem, and second it needs a retrocausality mechanism to look ahead in the queue and self destroy if the problem has been framed incorrectly. With such a treatment the problem stops being about the combinatorics of searching problems and solutions, but more of a problem of framing, which is easier to work with - and the human mind, with its capacity for universal function approximation and ability to switch perspectivs (when the conditions are right), is better than the normal UTM so as long as the human mind doesent alienate itself into some prideful alignment with its own specification, and refuses to "die" to itself - figuratively speaking. Toddlers and babies are as such the ideal model to base a new UTM with, as we know a few things about them. As a computational model they are resoruce efficient because they are biological; they are in a perpetual process of "dead" by sleeping a lot, and they dont carry arond the cybernetic specifications that makes people appear dumb over not being able to think "outside the box" - proverbially using things like hammers as chainsaws. Babies dont compute or follow algorithmic specifications to learn all the inmensely difficult intricacies of human life; they just reorganize and anneal until randomly converging to a global minimum by measure of how the data sampled from the problem naturally substrates itself: they are fully plastic; fully soil and water readily molded by the frame of the problem, instead of the subject corrupting the problem into a limit by framing it. They cheat the retroactive mechanism, by approaching the future framing of a problem, from an eternal past. Babies learning doesent even attempt computation. They just create the ideal substrate, and they plant the seed for the solution to be retrieved almost instantly later when they come of a certain age. 

Besides the general intuitions, i think a better model for computation is readily available in the idea of density and substrate, as it can be applied to different modalities of data.


https://www.youtube.com/watch?v=6Pqz890PyR0


Thanks for reading.




