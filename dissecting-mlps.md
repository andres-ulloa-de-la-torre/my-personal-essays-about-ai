
# Dissecting the emergent complexity of Multi-Layer-Perceptrons: why they work at all?

 If you did a PhD in AI you probably heard this over-and-over by fellow AI people and mathematicians working on chaos theory: MLPs during their function approximation are pulled towards emergent attractors that self organize the net in a non-linear fashion: we dont understand it very well besides the math that gives the net the ingredients to “just work”

So we are not exactly engineering neural networks, we are farming them out of the correct soup of ingredients; conceptual imagery which brings me back to 2019 when must industry practicioners (me included) where hyperparameter monkeys, using metrics as feedback in pytorch workflows. The terminal open in monitor A, and tensorboard open in monitor B. That was glorified by some media outlet (can’t remember which, but it was an Ivy League university journal) as the sexiest job in the world. Right.

Neural networks are hard to pin down to their precise nature because they are mathematical artifacts - asbtractions that at precision can only be described using mathematical components. But conversely because they exist at a high layer of human abstraction, they can be “casted” into a lower and partial form to manipulate a grounded perspective of them. Like backprop does by iterating a batch. And do this many times until some conceptual mastery is achieved. So mathematically the pattern of the neuron esentially comes down to the definition of the linear model, which is broken down in three components: basis function, weights+bias, and mapping. Whats challenging is understanding why interlinking this mapping many times gives arise to so much substance. The reason is not the mapping itself but the data.

Back in 2020 when I was twenty four years old i started to indepently research osbcure theories of pscyhometry, hunting the elusive jungian functions, using data mining on text rich environments like Reddit and BERT embeddings. Back then, by diving deeper into the research practice I was eventually convinced that most ML models were actually worthless. To me the discriminatory function and the hyperplane where superflous, and the best posible model was the simplest one: a model that fully embraces the instance of the data as it is. As you guessed right, im talking about KNN. The all-mighty KNN and still widely used today, in its bang average simplicity beating from my perspective another allmighty yet bang-average format: but not quite really. I by the way thought the kohonen self organizing map was a very slick algorithm for this reason, as it distantly attempted to model neurons without the linear model. Very cool stuff.

But I digress. The problem understanding neural networks lies in the notion of their boundaries for outside and inside: more specifically whats “inside”. Which naturally and intuitively is what the weight is for. Current models measure up to hundreds of gigabytes for this reason; they are containing the weights in their interior plus some architected composition of mappings: but weighting exactly what? Thing is, data is not homogenous and a taxonomy of it is required, or at least an intuition of it so neural networks start making sense.

Data mapped as an embedding is dense, but no all data is equally dense. When word2vector was done by SLMs there wasnt an absolute taxonomy to how dense data was. It was all relative to the geometry of the specific neural network modelling it. Right now you still get something I call “density relativism”, but  its less because the transformers of 2025 are starting to scale to the full size of the internet in its semantic diversity. So the data, starts to ground itself in the net into a shape that more accurately portrays whats outside. A small neural network for this reason is nothing but an artifact that transfer its inherent functional space (maximizing the “inside” in the dichotomy of outside vs inside), and geometrical topology unto the data its modelling. LLMs are different because their topology, regardless of architecture, after the training is starting to get really generic. 
And thats extremely good, because diving into the weights and analyzing them starts getting value. But what is actually going on in there? What happens if we start inquiring about weights the way a physicist would?

I’n my quest for the ultimate pscyhometric theory opening SLM brains, I eventually figured out two things: 1) the contents of the world seem to be universally mapped into the language of harmonics without losing any information, because oscillators when described in integrative, derivative and proportional  terms (a more general case of the linear model) are extremely universal, and you get a full working PID in neural nets when you add the harmonic motion of backpropagation during training 2) theres four fundamental operators for data-motion, that can be built into more complex oscillator forms. I baptized them: up, down, left and right. These operators are present routing a trace of logits in the hidden layer during inference, and are extremely active during training. 

Esentially this is what its going on. The dot product universally used to query an individual linear model is yielding a range that accounts in divisions for one of these four properties. The basic rules of these operators are: left likes to hang around with up, up hates down, down hates up, right doesent interact at all with left...etc etc. You get the idea. Esentially you get to cast data from the orignal language to the language of weights using this building blocks that all have mutual compatibilites, incompatibilities, and are capable of associating themselves together to create a bigger operator. Frame it likes this: neural networks are actually trees where data either: roots, trunks, and then yields fruit in the output layer in different layers of density: the closer to the dirt, the higher the density and the associated operators. But trees are all made of huge ammount of diminutive versions of the tree itself. So the neural network when disgustingly thought about as a linear algbera operation, gives no insight at all as to how they work looking at them in human terms. They are actually routing, and the operators simply act like arrows where continuity is achieved when all the arrows are poiting in compatible “highways” capable of making the data transit through the network without information loss. Same arrow in the trace? Thats called wave resonance. Oppsitional arrow? Destruction or distortion. If theres information loss, the backprop, which is a special case of the PID loop, makes the data try again in a different combination of arrows that allow passage.

I hope this brings some insight as to the concepts tackled in this very short essay, and what I’m currently interested in.

Thanks for readings.
